{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "457d361d",
   "metadata": {},
   "source": [
    "# Tokenizing with code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "774f85fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-4.1-mini\")\n",
    "\n",
    "tokens = encoding.encode(\"Hi my name is Muhammad and I like banoffee pie\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "595fb6c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[12194, 922, 1308, 382, 67641, 326, 357, 1299, 9171, 26458, 5148]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aca788b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12194 = Hi\n",
      "922 =  my\n",
      "1308 =  name\n",
      "382 =  is\n",
      "67641 =  Muhammad\n",
      "326 =  and\n",
      "357 =  I\n",
      "1299 =  like\n",
      "9171 =  ban\n",
      "26458 = offee\n",
      "5148 =  pie\n"
     ]
    }
   ],
   "source": [
    "for token_id in tokens:\n",
    "    token_text = encoding.decode([token_id])\n",
    "    print(f\"{token_id} = {token_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61cf373c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' and'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding.decode([326])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea41e9a",
   "metadata": {},
   "source": [
    "# The Illusion of \"memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69caaf0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "OLLAMA_BASE_URL = \"http://localhost:11434/v1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "574c7165",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# openai = OpenAI()\n",
    "ollama = OpenAI(base_url=OLLAMA_BASE_URL, api_key='ollama')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a3ef5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hi! I'm Muhammad!\"}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01c8f886",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"As-salamu alaykum, Muhammad! (peace be upon you) It's great to meet you. How can I assist you today? Do you have any questions or topics you'd like to discuss? I'm all ears and here to help!\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = ollama.chat.completions.create(model=\"llama3.2\", messages=messages)\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3fc41d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "    {\"role\": \"user\", \"content\": \"What's my name?\"}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "88acd789",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I don't have any information about you, so I'm not sure what your name is. We just started chatting, and I don't have any context or personal data about you. Would you like to share your name with me?\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = ollama.chat.completions.create(model=\"llama3.2\", messages=messages)\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f2a296",
   "metadata": {},
   "source": [
    "### wha??\n",
    "every call to an LLM is completely STATELESS. It's a totally new call, every single time. As AI engineers, it's OUR JOB to devise techniques to give the impression that the LLM has a \"memory\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd4d4d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hi! I'm Muhammad!\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"As-salamu alaykum, Muahmmad! (peace be upon you) It's a pleasure to meet you. How can I assist you today? Is there anything on your mind that you'd like to talk about or ask about?\"},\n",
    "    {\"role\": \"user\", \"content\": \"What's my name?\"}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b27b624",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your name is Muhammad.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = ollama.chat.completions.create(model=\"llama3.2\", messages=messages)\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5afb37",
   "metadata": {},
   "source": [
    "1. Every call to an LLM is stateless\n",
    "2. We pass in the entire conversation so far in the input prompt, every time\n",
    "3. This gives the illusion that the LLM has memory - it apparently keeps the context of the conversation\n",
    "4. But this is a trick; it's a by-product of providing the entire conversation, every time"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-engineering",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
